{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiac-generator",
   "metadata": {},
   "source": [
    "# *Nb #3* - Memory management: shedding off excess time\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Up to now we have learned how to write a kernel function and how to efficiently run it. Our input data progressed together with our knowledge as we switched from a 1D sampled signal to a 2D image array at the input / output of our kernel. Now it's time to go back to 1D data samples, while still potentially leveraging a 2D grid of threads. We will also look at how we can recycle existing threads and use them in different contexts within the same kernel. However, before we do so there is still one elephant in the room that is keeping our kernel functions from reaching their full potential - memory management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-reflection",
   "metadata": {},
   "source": [
    "## First steps towards optimal memory access\n",
    "\n",
    "### What goes on in the background?\n",
    "\n",
    "At the end of the third assignment there was a hint regarding the first step of memory management. Calling the statement `cuda.to_device()` enables us to allocate memory on the GPU and enjoy short read/write times, instead of always passing CPU-based arrays to the kernel function. One can think of it as a `malloc()` statement in C, with the host being the GPU's RAM. Judging by the below figure, transferring the entire array only once to and from the GPU is far more efficient than constantly referencing objects in the CPU's working memory. Moreover, if numerous therads access the data bus at the same time, the bus becomes a bottleneck, stalling any processing instructions and increasing the time delays.\n",
    "\n",
    "Keep in mind that Numba does implement conservative memory transfers in the background. These take place during kernel invocation, initially transferring memory to the GPU and back to the CPU once the kernel finishes. However, in certain cases it's more rewarding to keep the data on the GPU in between multiple computations.\n",
    "\n",
    "<img src=\"./Image/cuda-workflow.png\" alt=\"CUDA workflow.\" style=\"width: 500px; padding: 20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-gregory",
   "metadata": {},
   "source": [
    "### Simple memory management in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-complement",
   "metadata": {},
   "source": [
    "Let's revisit the 2D histogram kernel from last time, re-measure its execution time, and see how simple memory migration will improve our GPU code. Run the below to import the relevant code and check that everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from Helpers import synchronous_kernel_timeit, get_lenna, kernel_2D\n",
    "\n",
    "\n",
    "_, image_array_gs = get_lenna() # Get the test images in ndarray form\n",
    "\n",
    "xmin, xmax = (0, 255)\n",
    "nbins = 255\n",
    "\n",
    "# Reset histogram\n",
    "histogram_out = np.zeros(nbins)\n",
    "\n",
    "# Wrap kernel funciton call and save to variable for later usage\n",
    "histogram_Lenna = lambda: kernel_2D[(4, 4), (16, 16)](image_array_gs, xmin, xmax, histogram_out)\n",
    "histogram_Lenna()\n",
    "\n",
    "# Calculate x-axis values for plotting reasons (we also need to recalculate the bin width)\n",
    "bin_width = (xmax - xmin) / nbins\n",
    "x_vals = np.linspace( xmin, xmax, nbins, endpoint=False ) + bin_width/2\n",
    "\n",
    "# Plot the histogram and image\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.bar( x_vals, histogram_out, width=bin_width, alpha=0.5 )\n",
    "ax1.plot( x_vals, histogram_out )\n",
    "ax2.imshow(image_array_gs, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-settlement",
   "metadata": {},
   "source": [
    "Let's refresh out memory and time the function again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-writer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_histogram = synchronous_kernel_timeit( histogram_Lenna, number=100 )\n",
    "print(t_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-rebecca",
   "metadata": {},
   "source": [
    "Now migrate part of the data to the **GPU's (device's) global memory** before running the kernel. In this example, we are still timing some memory access operations, but the largest portion of data has been copied to the GPU. The memory allocation and copying are both achieved by calling `cuda.to_device()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "image_array_gs_device = cuda.to_device(image_array_gs)\n",
    "\n",
    "histogram_Lenna_dev = lambda: kernel_2D[(4, 4), (16, 16)](image_array_gs_device, xmin, xmax, histogram_out)\n",
    "t_histogram_dev = synchronous_kernel_timeit( histogram_Lenna_dev, number=100 )\n",
    "print(t_histogram_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-tragedy",
   "metadata": {},
   "source": [
    "Faster. We can observe that a considerable amount of time is consumed for **accessig data in the CPU's (host's) RAM**. However, the memory migration also induces a time delay. Let's evaluate that next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "t_mem = synchronous_kernel_timeit( lambda: cuda.to_device(image_array_gs), number=100 )\n",
    "print(\"Time needed fo read-only memory migration (s)\", t_mem)\n",
    "print(\"Sum of memory migration and processing times (s)\", t_mem + t_histogram_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-fellow",
   "metadata": {},
   "source": [
    "### Increasing the gains: chaining functions\n",
    "\n",
    "So, timing the kernel that uses the pre-allocated memory yields a better result, but the total time stays rougly the same. Memory migration starts outpacing unoptimized code when a chain of operations on the GPU uses the same data. Let's look at the following example:\n",
    "\n",
    "1. The input image gets inverted\n",
    "2. The inverse is flattened\n",
    "3. The histogram of the flattened image is calculated\n",
    "\n",
    "Step by step, the below cells calculate the individual contributions with and without memory migration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-contemporary",
   "metadata": {},
   "source": [
    "First, processing the inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import invert_image\n",
    "\n",
    "image_array_gs_inv = np.empty_like(image_array_gs)\n",
    "\n",
    "inversion_Lenna = lambda: invert_image[(4, 4), (16, 16)](image_array_gs, image_array_gs_inv)\n",
    "t_inversion = synchronous_kernel_timeit( inversion_Lenna, number=100 )\n",
    "print(t_inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array_gs_inv_device = cuda.to_device(image_array_gs)\n",
    "\n",
    "inversion_Lenna_dev = lambda: invert_image[(4, 4), (16, 16)](image_array_gs_device, image_array_gs_inv_device)\n",
    "t_inversion_dev = synchronous_kernel_timeit( inversion_Lenna_dev, number=100 )\n",
    "print(t_inversion_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-cursor",
   "metadata": {},
   "source": [
    "Next up the flattening of the image, using the inverted image as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import flatten_image\n",
    "\n",
    "image_array_gs_flat = np.zeros(image_array_gs.shape)\n",
    "\n",
    "flatten_Lenna = lambda: flatten_image[(4, 4), (16, 16)](image_array_gs_inv, image_array_gs_flat)\n",
    "t_flatten = synchronous_kernel_timeit( flatten_Lenna, number=100 )\n",
    "print(t_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array_gs_flat_device = cuda.to_device(image_array_gs_flat)\n",
    "\n",
    "flatten_Lenna_dev = lambda: flatten_image[(4, 4), (16, 16)](image_array_gs_inv_device, image_array_gs_flat_device)\n",
    "t_flatten_dev = synchronous_kernel_timeit( flatten_Lenna_dev, number=100 )\n",
    "print(t_flatten_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-proposal",
   "metadata": {},
   "source": [
    "And finally sum the individual contributions to see how much time the execution of such a program would take on average. Taking the previously established histogram and memory migration timing results yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without memory management (s):\", t_histogram + t_inversion + t_flatten)\n",
    "print(\"Using basic memory management (s):\", t_mem + t_histogram_dev + t_inversion_dev + t_flatten_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-complexity",
   "metadata": {},
   "source": [
    "Recall that during the previous assignments, we saw the GPU doesn't always outpace the CPU, even in easily paralellizable code execution. The above results hint as to why that is the case. In fact, **the GPU performs well when there is a single migration of a dataset, followed by a the successive execution of complex, potentially iterative, and paralellizable algorithms.** \n",
    "\n",
    "When frequent read/write operations are needed on small portions of data that is updated by other processes in real-time, then the CPU might be a better option. There is no magic formula. When possible, consider the pros and cons of all available architectures, run some initial tests, and go for the best performing option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-listening",
   "metadata": {},
   "source": [
    "Lastly, let's see the results of the timed code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-elephant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset and compute all values\n",
    "histogram_out = np.zeros(nbins)\n",
    "#histogram_Lenna_dev()\n",
    "inversion_Lenna_dev()\n",
    "flatten_Lenna_dev()\n",
    "kernel_2D[(4, 4), (16, 16)](image_array_gs_flat_device, xmin, xmax, histogram_out)\n",
    "\n",
    "# Plot the results\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(12,4))\n",
    "plt.subplots_adjust(left=0, top=1, right=1, bottom=0, wspace=0.5)\n",
    "ax1.imshow(image_array_gs, cmap='gray'); ax1.set_title('Reference image')\n",
    "ax2.imshow(image_array_gs_inv, cmap='gray'); ax2.set_title('Invert')\n",
    "ax3.imshow(image_array_gs_flat, cmap='gray', vmin=0, vmax=255); ax3.set_title('Flatten')\n",
    "ax4.bar( x_vals, histogram_out, width=bin_width, alpha=0.5 )\n",
    "ax4.plot( x_vals, histogram_out ); ax4.set_title('Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-model",
   "metadata": {},
   "source": [
    "## Memory tiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-female",
   "metadata": {},
   "source": [
    "### Spoiled for choice\n",
    "\n",
    "Of ocurse there isn't just one memory type that we can access on the GPU. Luck has it we are quite flexible in the way we choose which memory we use and when. In fact, there are six different tiers of memory on the GPU at the disposal of its threads. The ones we are concerted with are: global memory (GMEM), constant memory (CMEM), and shared memory (SMEM). \n",
    "\n",
    "<img src=\"./Image/memory-table.png\" alt=\"CUDA memory table.\" style=\"width: 800px; padding: 20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-preserve",
   "metadata": {},
   "source": [
    "Both GMEM and SMEM reside somewhat further away from the the stream multiprocessor (SM), and therefore the CUDA cores, while the SMEM is present on the SM itself and is therefore adjacent to the CUDA cores. Its downside is that it's limited in size. Only 48 kB on the GT 730. That's less than the size of a 128x128 px RGB (8 bit color), so we need to be frugal in the way we use it. If we only need to read the data, then CMEM is a better option. It has more space at its disposal, and although its positioned further away from the SM, a portion of it is autmatically cached on the SM. Lastly, for large amounts of data and flexible read/write operations, GMEM is the way to go.\n",
    "\n",
    "<img src=\"./Image/memory-architecture.png\" alt=\"CUDA architecture.\" style=\"width: 600px; padding: 20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-generator",
   "metadata": {},
   "source": [
    "### Using more memory tiers in practice\n",
    "\n",
    "Redefining the kernel to support the additional memory types is a slightly more tedious process. The below kernel definition roughly does the following:\n",
    "1. Define constant (entire GPU) and shared memory (per block), together with the already familiar thread indexes and strides\n",
    "2. Set the shared memory to zeros\n",
    "3. **Synchronize** threads in block\n",
    "4. Execute te known histogram function\n",
    "5. **Synchronize** threads in block\n",
    "6. Migrate the data from shared back to global memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, types\n",
    "\n",
    "\n",
    "NBINS = 255 # Defined in advance for the allocation of SMEM\n",
    "            \n",
    "            \n",
    "@cuda.jit\n",
    "def kernel(samples, xmin, xmax, histogram_out):\n",
    "    '''Use the GPU for generateing a histogram of 2D input data.'''\n",
    "\n",
    "    # Transfer the samples into CMEM and initialize SMEM\n",
    "    CMEM = cuda.const.array_like(samples) # In general, for all blocks\n",
    "    smem = cuda.shared.array(shape=NBINS, dtype=types.uint32) # Per-block\n",
    "\n",
    "    # Calculate the thread's absolute position within the grid\n",
    "    x = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    y = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "    # Set stride equal to the number of threads we have available in either direction\n",
    "    stride_x = cuda.gridDim.x * cuda.blockDim.x\n",
    "    stride_y = cuda.gridDim.y * cuda.blockDim.y\n",
    "\n",
    "    # Calc the resolution of the histogram\n",
    "    bin_width = (xmax - xmin) / NBINS\n",
    "\n",
    "    \n",
    "    # Initialize the SMEM array to zeros\n",
    "    if cuda.threadIdx.y == 0: # Execute only once per block (SMEM scope)\n",
    "        for i in range(cuda.threadIdx.x, NBINS, cuda.blockDim.x): # Stride on a per-block basis till NBINS is reached\n",
    "            smem[i] = 0\n",
    "\n",
    "            \n",
    "    cuda.syncthreads() # Wait for initialization\n",
    "\n",
    "    \n",
    "    for i in range(x, CMEM.shape[0], stride_x):\n",
    "        for j in range(y, CMEM.shape[1], stride_y):\n",
    "            # Associate each sample in the interval [xmin, xmax) with a bin and update the histogram. Skip outliers.\n",
    "            bin_number = int((CMEM[i, j] - xmin) / bin_width)\n",
    "            if bin_number >= 0 and bin_number < smem.shape[0]:\n",
    "                # cuda.atomic.add(histogram_out, bin_number, 1)  # Prevent race conditions\n",
    "                cuda.atomic.add(smem, bin_number, 1)  # Prevent race conditions\n",
    "\n",
    "                \n",
    "    cuda.syncthreads() # Wait for the summations to finish\n",
    "\n",
    "    \n",
    "    # Transfer data back from SMEM to host or device memory\n",
    "    if cuda.threadIdx.y == 0: # Execute only once per block (SMEM scope)\n",
    "        for i in range(cuda.threadIdx.x, NBINS, cuda.blockDim.x): # Stride on a per-block basis till NBINS is reached\n",
    "            # Each block writes to a NBINS-long sub-division of the output histogram\n",
    "            histogram_out[ i + cuda.blockIdx.x * NBINS, cuda.blockIdx.y ] = smem[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-demographic",
   "metadata": {},
   "source": [
    "### Executing a kernel that uses shared memory\n",
    "\n",
    "The newly defined kernel requires one additional step before and after running it. Since the **shared memory is allocated on a per-block basis**, the histogram must be cloned to every block. This is achieved by multilying the histogram's size by the number of blocks in the x-dimension. Tile the resulting sequences in the y-dimenison of the grid of blocks.\n",
    "\n",
    "After the blocks populate their corresponding parts of the extended histogram array, the sub-divisions must be summed up into a single historam.\n",
    "\n",
    "The below code implements the two additional steps and runs the kernel. We can visually examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "blks_x, blks_y = (4, 4)\n",
    "th_x, th_y = (8, 8)\n",
    "\n",
    "# Reset histogram\n",
    "# Each block writes to a NBINS-long sub-division of the output histogram. Repeat across 2-axis for Y-dimension.\n",
    "histogram_out_ext = np.zeros(( NBINS * blks_x, blks_y ), dtype=np.uint32)\n",
    "\n",
    "# Allocate memory on the device (GPU) and copy the arrays\n",
    "image_array_gs_device = cuda.to_device(image_array_gs)\n",
    "histogram_out_ext_device = cuda.to_device(histogram_out_ext)\n",
    "\n",
    "# Execute kernel\n",
    "kernel[(blks_x, blks_y), (8, 8)](image_array_gs_device, xmin, xmax, histogram_out_ext_device)\n",
    "\n",
    "# Copy the output back from the device (GPU) to the host (CPU)\n",
    "histogram_out_ext_device.copy_to_host(histogram_out_ext)\n",
    "\n",
    "\n",
    "# Combine the results of individual blocks back into a single histogram\n",
    "if len(histogram_out_ext.shape) == 2: histogram_out_ext = np.sum(histogram_out_ext, axis=1) # Compress Y-dim.\n",
    "histogram_out = np.sum(np.array_split(histogram_out_ext, blks_x, axis=0), axis=0) # Compress X-dim.\n",
    "\n",
    "\n",
    "# Calculate x-axis values for plotting reasons (we also need to recalculate the bin width)\n",
    "bin_width = (xmax - xmin) / NBINS\n",
    "x_vals = np.linspace( xmin, xmax, NBINS, endpoint=False ) + bin_width/2\n",
    "\n",
    "# Plot the histogram and image\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.bar( x_vals, histogram_out, width=bin_width, alpha=0.5 )\n",
    "ax1.plot( x_vals, histogram_out )\n",
    "ax2.imshow(image_array_gs, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-feeding",
   "metadata": {},
   "source": [
    "The last thing therefore is to assess the reuslting time savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import synchronous_kernel_timeit as sync_timeit\n",
    "from Helpers import kernel_2D as kernel_unoptimized\n",
    "\n",
    "\n",
    "# Make sure both are pre-compiled\n",
    "kernel[(blks_x, blks_y), (th_x, th_y)](image_array_gs_device, xmin, xmax, histogram_out_ext_device)\n",
    "kernel_unoptimized[(blks_x, blks_y), (th_x, th_y)](image_array_gs, xmin, xmax, histogram_out)\n",
    "\n",
    "t1 = sync_timeit( \n",
    "    lambda: kernel[(4, 4), (8, 8)](image_array_gs_device, xmin, xmax, histogram_out_ext_device), \n",
    "    number=10 )\n",
    "t2 = sync_timeit( \n",
    "    lambda: kernel_unoptimized[(4, 4), (8, 8)](image_array_gs, xmin, xmax, histogram_out), \n",
    "    number=10 )\n",
    "\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-wagner",
   "metadata": {},
   "source": [
    "## Assignment - Optimized memory access\n",
    "\n",
    "\n",
    "### Report and deadline\n",
    "\n",
    "*Follow the bellow insructions and write a report when done. Explain what you have learned and critically assess the performance of your code. The report shouldn't exceed 3 pages (including figures). If you wish to share your code, include a link to the corresponding repository in your report or write the report in the form of a Notebook.*\n",
    "\n",
    "*Hand in your reports to alexander.marinsek@kuleuven.be by:*\n",
    "* *SA & GS: Sunday, 09/05/2021, 23:59 CEST*\n",
    "* *ES: Sunday, 16/05/2021, 23:59 CEST*\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "##### I. Kolmogorovâ€“Zurbenko filter\n",
    "1. Designe a GPU kernel funcitons that implements the Kolmogorov-Zurbenko filter. \n",
    "    - Keep speed of execution in mind when designing it (parallel sample access, atomic operations, parallel summation, etc.)\n",
    "    - Test it using the input signal of your liking, and try to adhere to the optimizations we know so far (warp size)\n",
    "2. Time the kernel with and without memory pre-allocation (`cuda.to_device()`). How much time do the two kernels take to execute and how much time is used up for memory migration?\n",
    "3. (optional) Leverage the other memory types on the GPU. What are the gains?\n",
    "    \n",
    "##### II. Including the DFT\n",
    "1. Verify that your DFT code from the first assignment can correctly process the input signal you defined. Change the two if needed.\n",
    "2. Now calculate the DFT of the filtered signal. Do so without migrating the data back to CPU memory in-between the two operations. Instead, keep it in the GPU's memory.\n",
    "3. Additionally calculate the DFT of the filter's input signal, based on the data that has been migrated to the GPU. The program now consists of the functions: DFT-ZK-DFT.\n",
    "4. Discuss the timing results and verify all of the function outputs are correct (DFT and filter).\n",
    "5. Compare the optimal timing results (II. 2.) with a non-optimized version of the code, where data is constantly transfers to and from the CPU. What are the gains?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-nation",
   "metadata": {},
   "source": [
    "### Helpers and pointers - Kolmogorov-Zurbenko (KZ) filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-doctrine",
   "metadata": {},
   "source": [
    "#### Equation\n",
    "\n",
    "$KZ_{m,k}[X(t)] = \\sum_{s=-k*(m-1)/2}^{k*(m-1)/2} X(t+s) \\cdot a_s^{m,k}$\n",
    "\n",
    "#### The ZK filer relies on two main operations\n",
    "- Multiplication of nearby samples with the corrresponding filter coefficients\n",
    "- Summation of the contribtions\n",
    "\n",
    "Both should reside within the same kernel function. Make sure to **synchronize the threads** between the two operations using `cuda.syncthreads()`.\n",
    "\n",
    "#### Other things to keep in mind\n",
    "- Pre-compute the filter coefficients using the supplied function\n",
    "- The smoothed sequence will be shorter by the number of filter coefficients, minus one. An optonal workaround is to pre-pad it.\n",
    "- `cuda.syncthreads()` works on a **per-block basis**. In this assignment, it is sufficient to use a single block, but if you implement striding, the input data can still be relatively large.\n",
    "- Start off with smaller $m$ & $k$ values (they determine the intermediate data size).\n",
    "- Parallel summation works best for $2^N$ elements - complexity reduciton from $\\mathcal{O}(N)$ to $\\mathcal{O}(log_2(N))$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-induction",
   "metadata": {},
   "source": [
    "The below figure outlines the main steps of one approach to implementing the ZK filter. \n",
    "\n",
    "Regardless of whether you define the intermediate data in one or more dimensions, keep in mind that the size of the intermediate data will, in general, surpass the size of the initial input and final output sequences.\n",
    "\n",
    "<img src=\"./Image/ZK-filter.svg\" alt=\"CUDA workflow.\" style=\"width: 800px; padding: 20px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kz_coeffs(m, k):\n",
    "    \"\"\"Calc KZ coefficients. Source https://github.com/Kismuz/kolmogorov-zurbenko-filter\"\"\"\n",
    "\n",
    "    # Coefficients at degree one\n",
    "    coef = np.ones(m)\n",
    "\n",
    "    # Iterate k-1 times over coefficients\n",
    "    for i in range(1, k):\n",
    "\n",
    "        t = np.zeros((m, m + i * (m - 1)))\n",
    "        for km in range(m):\n",
    "            t[km, km:km + coef.size] = coef\n",
    "\n",
    "        coef = np.sum(t, axis=0)\n",
    "\n",
    "    assert coef.size == k * (m - 1) + 1\n",
    "\n",
    "    return coef / m ** k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-office",
   "metadata": {},
   "source": [
    "### Helpers and pointers - alternative plotting options\n",
    "\n",
    "As a substitute to Matplotlib, you can use Plotly. It provides a set of handy features, making it easier to interract with the plotted data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.linspace(0, 2*np.pi, 100)\n",
    "y_data_1 = np.sin(x_data)\n",
    "y_data_2 = np.cos(x_data)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_traces(go.Scatter( x=x_data, y=y_data_1, name='Sin'))\n",
    "fig.add_traces(go.Scatter( x=x_data, y=y_data_2, name='Cos'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-pierre",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "* (Maximizing memory throughput) CUDA programming guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#maximize-memory-throughput\n",
    "* (Numba memory management) Numba docs: https://numba.pydata.org/numba-doc/latest/cuda/memory.html\n",
    "* (Kolmogorov-Zurbenko filter) Wikipedia: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Zurbenko_filter\n",
    "* (Peculiar constant memory usage in Numba) Stackoverflow, Github issue, and Numba docs: \n",
    "    * https://stackoverflow.com/questions/63311574/in-numba-how-to-copy-an-array-into-constant-memory-when-targeting-cuda\n",
    "    * https://github.com/numba/numba/issues/4057\n",
    "    * https://numba.pydata.org/numba-doc/latest/cuda-reference/kernel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-father",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
