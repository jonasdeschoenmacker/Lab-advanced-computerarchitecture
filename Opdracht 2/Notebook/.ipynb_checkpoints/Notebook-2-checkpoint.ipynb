{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serial-assessment",
   "metadata": {},
   "source": [
    "# *Nb #2* - Multidimensional data: how many threads can / should we use?\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Last time we looked at how to write a kernel and what the performance benefits of using a GPU over a CPU are. For both the test code and the assignment we primarilly used threads spread out in a linear grid (x direction). That's a big improvement over sequential processing, but if the data stretches multiple dimensions, it sometimes makes sense to reflec that in the spatial distribution of threads. \n",
    "\n",
    "In this notebook we will look at how to use threads and blocks in a 2D grid, what happens when we run out of threads, and how to overcome that without hardcoding. We will also look at the GPU's architecture more in detailm based on which you will establish an optimal thread count during your assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-manual",
   "metadata": {},
   "source": [
    "## Striding\n",
    "\n",
    "Imagine having a large dataset and an insufficient number of threads to cover it entirely. To avoid loosing the overhead data or manually calling the kernel on several parts of the data, striding is used to assign an a single thread to multiple samples of the dataset. Since now a single thread has to loop over several values, striding will also increase the execution time of the kernel. \n",
    "\n",
    "\n",
    "<img src=\"./Image/striding.png\" alt=\"Striding overview.\" style=\"width: 750px; padding: 20px\"/>\n",
    "\n",
    "\n",
    "To show this in practice, let's first recap on what we did in the previous notebook before adding striding to the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "miniature-phrase",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "import numpy as np # Arrays in Python\n",
    "from matplotlib import pyplot as plt # Plotting library\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def kernel_parallel(samples, xmin, xmax, histogram_out):\n",
    "    '''Use the GPU for generateing a histogram. In parallel.'''\n",
    "\n",
    "    # Calculate the thread's absolute position within the grid\n",
    "    x = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    # Calc the resolution of the histogram\n",
    "    nbins = histogram_out.shape[0]\n",
    "    bin_width = (xmax - xmin) / nbins\n",
    "    \n",
    "    # Associate each sample in the interval [xmin, xmax) with a bin and update the histogram. Skip outliers.\n",
    "    bin_number = int((samples[x] - xmin) / bin_width)\n",
    "    if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
    "            cuda.atomic.add(histogram_out, bin_number, 1) # Prevent race conditions\n",
    "\n",
    "            \n",
    "NUMBER_OF_SAMPLES = 8_192\n",
    "            \n",
    "\n",
    "# Repeatable results\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the observed signal\n",
    "signal = np.random.normal(size=8_192, loc=0, scale=1).astype(np.float32)\n",
    "\n",
    "            \n",
    "# Define the range\n",
    "xmin = -4\n",
    "xmax = 4\n",
    "\n",
    "# Number of histogram elements = bins\n",
    "nbins = 100\n",
    "\n",
    "# Initiate an empty histogram\n",
    "histogram_out = np.zeros(nbins)\n",
    "\n",
    "threads = 512 # Per block\n",
    "blocks = int(NUMBER_OF_SAMPLES / threads) # Cover all of the data\n",
    "\n",
    "# We're not timing the function, so just call it once to get the histogram\n",
    "kernel_parallel[blocks, threads](signal, xmin, xmax, histogram_out)\n",
    "\n",
    "\n",
    "# Calculate x-axis values for plotting reasons (we also need to recalculate the bin width)\n",
    "bin_width = (xmax - xmin) / nbins\n",
    "x_vals = np.linspace( xmin, xmax, nbins, endpoint=False ) + bin_width/2\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar( x_vals, histogram_out, width=bin_width )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-carolina",
   "metadata": {},
   "source": [
    "Okay, that all works nicely. Now let's pretend that we can only use 4 blocks, giving us a total of 2048 threads. We do that by setting the block count to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "transparent-palace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset histogram\n",
    "histogram_out = np.zeros(nbins)\n",
    "\n",
    "# Set the number of thread blocks to 4\n",
    "blocks = 4 \n",
    "\n",
    "# We're not timing the function, so just call it once to get the histogram\n",
    "kernel_parallel[blocks, threads](signal, xmin, xmax, histogram_out)\n",
    "\n",
    "\n",
    "# Calculate x-axis values for plotting reasons (we also need to recalculate the bin width)\n",
    "bin_width = (xmax - xmin) / nbins\n",
    "x_vals = np.linspace( xmin, xmax, nbins, endpoint=False ) + bin_width/2\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar( x_vals, histogram_out, width=bin_width )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-spencer",
   "metadata": {},
   "source": [
    "Not the result we were expecting any more! That's because only a portion of the samples were iterated. Now let's implement a stride loop. Our goal is to run each thread $N$-times, where:\n",
    "\n",
    "$N = \\dfrac{Number\\_of\\_samples}{Number\\_of\\_threads}$\n",
    "\n",
    "In our case $N=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def kernel_parallel_strided(samples, xmin, xmax, histogram_out):\n",
    "    '''Use the GPU for generateing a histogram. In parallel.'''\n",
    "\n",
    "    # Calculate the thread's absolute position within the grid\n",
    "    x = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    # Calc the resolution of the histogram\n",
    "    nbins = histogram_out.shape[0]\n",
    "    bin_width = (xmax - xmin) / nbins\n",
    "    \n",
    "    # Set stride equal to the number of threads we have available in the x-direction\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "    \n",
    "    for i in range(x, samples.shape[0], stride):\n",
    "        # Associate each sample in the interval [xmin, xmax) with a bin and update the histogram. Skip outliers.\n",
    "        bin_number = int((samples[i] - xmin) / bin_width) # 'samples[i]' instead of 'samples[x]'\n",
    "        if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
    "                cuda.atomic.add(histogram_out, bin_number, 1) # Prevent race conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-walnut",
   "metadata": {},
   "source": [
    "In the above kernel each thread attends to the sample at it's own index (`x`), the sample at `x + stride`, `x + 2*stride`, `x + n*stride` ... as long as `x + n*stride` actually represents an index of a sample. In other words, the stride loop will run sequentially till we run our of samples.\n",
    "\n",
    "There are two main benefits to using the stride loop:\n",
    "1. If a sufficient number of threads is used, the loop only executes once and the execution time remains small\n",
    "2. Should there be too many threads, the statement also prevents the excess threads from writing to other parts of the memory\n",
    "\n",
    "Now let's calculate the histogram again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset histogram\n",
    "histogram_out = np.zeros(nbins)\n",
    "\n",
    "# We're not timing the function, so just call it once to get the histogram\n",
    "kernel_parallel_strided[blocks, threads](signal, xmin, xmax, histogram_out)\n",
    "\n",
    "\n",
    "# Calculate x-axis values for plotting reasons (we also need to recalculate the bin width)\n",
    "bin_width = (xmax - xmin) / nbins\n",
    "x_vals = np.linspace( xmin, xmax, nbins, endpoint=False ) + bin_width/2\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar( x_vals, histogram_out, width=bin_width )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-safety",
   "metadata": {},
   "source": [
    "## Processing multidimensional data\n",
    "\n",
    "Up to this point we've only considered an input stream of samples. In the following example we will switch from a 1D array to a 2D array by calculating the histogram of an image. First import the test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Fetch image from URL\n",
    "url = 'https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Convert image to numpy array\n",
    "image_array = np.asarray(image, dtype=np.int64)\n",
    "\n",
    "# Convert to grayscale\n",
    "image_array_gs = np.sum(np.asarray(image), axis=2) / 3\n",
    "\n",
    "print(image_array_gs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-solution",
   "metadata": {},
   "source": [
    "#### Using a 2D grid (grayscale)\n",
    "\n",
    "Th original image is 3D (RGB), so we've bluntly converted it to grayscale. After checking it's output dimensions are 2D, proceed with the new kernel definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def kernel_2D(samples, xmin, xmax, histogram_out):\n",
    "    '''Use the GPU for generateing a histogram of 2D input data.'''\n",
    "\n",
    "    # Calculate the thread's absolute position within the grid\n",
    "    x = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    y = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    \n",
    "    # Set stride equal to the number of threads we have available in either direction\n",
    "    stride_x = cuda.gridDim.x * cuda.blockDim.x\n",
    "    stride_y = cuda.gridDim.y * cuda.blockDim.y\n",
    "    \n",
    "    # Calc the resolution of the histogram\n",
    "    nbins = histogram_out.shape[0]\n",
    "    bin_width = (xmax - xmin) / nbins\n",
    "    \n",
    "    for i in range(x, samples.shape[0], stride_x):\n",
    "        for j in range(y, samples.shape[1], stride_y):\n",
    "            # Associate each sample in the interval [xmin, xmax) with a bin and update the histogram. Skip outliers.\n",
    "            bin_number = int((samples[i,j] - xmin) / bin_width) \n",
    "            if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
    "                    cuda.atomic.add(histogram_out, bin_number, 1) # Prevent race conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-theater",
   "metadata": {},
   "source": [
    "The kernel can now handle threads in both X and Y direction. We introduced another stride loop for the Y-direction and the data is now interfaced in both dimensions: `sample[i,j]`. We could omit both stride loops and write `sample[x,y]` if we were certain that the number of thread grid size will always match the shape of the dataset. Often that's not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = (0, 255)\n",
    "nbins = 255\n",
    "\n",
    "# Reset histogram\n",
    "histogram_out = np.zeros(nbins)\n",
    "\n",
    "# We're not timing the function, so just call it once to get the histogram\n",
    "kernel_2D[(4, 4), (16, 16)](image_array_gs, xmin, xmax, histogram_out)\n",
    "\n",
    "\n",
    "# Calculate x-axis values for plotting reasons (we also need to recalculate the bin width)\n",
    "bin_width = (xmax - xmin) / nbins\n",
    "x_vals = np.linspace( xmin, xmax, nbins, endpoint=False ) + bin_width/2\n",
    "\n",
    "# Plot the histogram and image\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.bar( x_vals, histogram_out, width=bin_width )\n",
    "ax2.imshow(image_array_gs, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import synchronous_kernel_timeit as sync_timeit\n",
    "\n",
    "t = sync_timeit( lambda: kernel_2D[(4, 4), (16, 16)](image_array_gs, xmin, xmax, histogram_out), number=10)\n",
    "\n",
    "print( t )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-cancellation",
   "metadata": {},
   "source": [
    "That's it! Regardles of how many threads we allocate to the task, they are going to cover the entire image. However, depending on their number, we can then expect the processing time to fluctuate accordingly.\n",
    "\n",
    "#### Using a 3D grid (RGB)\n",
    "\n",
    "Taking the same approach to increase the grid dimensions, let's now calculate the histogram of the RGB image in parallel. When calling the kernel we leave the number of threads per kernel untouched and add two more layers of thread blocks in the Z-axis. Consequently we get tripple the amount of threads, compared to the grayscale example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def kernel_3D(samples, xmin, xmax, histogram_out):\n",
    "    '''Use the GPU for generateing a histogram of 3D input data (RGB).'''\n",
    "\n",
    "    # Calculate the thread's absolute position within the grid\n",
    "    x = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    y = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "    z = cuda.threadIdx.z + cuda.blockIdx.z * cuda.blockDim.z\n",
    "\n",
    "    # Set stride equal to the number of threads we have available in either direction\n",
    "    stride_x = cuda.gridDim.x * cuda.blockDim.x\n",
    "    stride_y = cuda.gridDim.y * cuda.blockDim.y\n",
    "    stride_z = cuda.gridDim.z * cuda.blockDim.z\n",
    "\n",
    "    nbins = histogram_out.shape[0]\n",
    "    bin_width = (xmax - xmin) / nbins\n",
    "\n",
    "    for i in range(x, samples.shape[0], stride_x):\n",
    "        for j in range(y, samples.shape[1], stride_y):\n",
    "            for k in range(z, samples.shape[2], stride_z):\n",
    "                bin_number = int((samples[i, j, k] - xmin) / bin_width)\n",
    "                if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
    "                    cuda.atomic.add(histogram_out, bin_number, 1)  # Prevent race conditions\n",
    "                    \n",
    "\n",
    "# Reset histogram\n",
    "histogram_out = np.zeros(nbins)\n",
    "\n",
    "# We're not timing the function, so just call it once to get the histogram\n",
    "kernel_3D[(4, 4, 3), (16, 16, 1)](image_array, xmin, xmax, histogram_out)\n",
    "\n",
    "\n",
    "# Plot the histogram and image\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.bar( x_vals, histogram_out, width=bin_width )\n",
    "ax2.imshow(image_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-stomach",
   "metadata": {},
   "source": [
    "Lastly, calculating the speed uncovers a slight anomaly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers import synchronous_kernel_timeit as sync_timeit\n",
    "\n",
    "t = sync_timeit( lambda: kernel_3D[(4, 4, 3), (16, 16, 1)](image_array, xmin, xmax, histogram_out), number=10)\n",
    "\n",
    "print( t )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-pittsburgh",
   "metadata": {},
   "source": [
    "Ouch! Although we made sure that every data sample had its own thread, the calculation time increased. Normally one expects this to happen when the 2D matrices of the three colors are processed in sequence. \n",
    "\n",
    "The reason for the time increase is that we are performing *safe* or *blocking* writeoperations on the output histogram array. In our case, we now have a 67 % increase in such operations (two additional color matrices). Therefore, the time increase is expected behaviour. What we have yet to determine is, how does the number of threads and kernels influence the end processing time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-authority",
   "metadata": {},
   "source": [
    "## Thread warps and CUDA cores\n",
    "\n",
    "And a small part of you imagination just dissapeared...\n",
    "\n",
    "<div style=\"display: flex\" >\n",
    "<img src=\"./Image/star-trek-warp.jpeg\" alt=\"Star trek.\" style=\"height: 150px; margin: 20px; float: left\"/>\n",
    "<img src=\"./Image/captain-kirk.jpg\" alt=\"Star trek.\" style=\"height: 150px; margin: 20px; float: left\"/>\n",
    "<img src=\"./Image/warp-knitting.png\" alt=\"Warp in knitting.\" style=\"height: 150px; margin: 20px; float: left\"/>\n",
    "<img src=\"./Image/flock-of-sheep.jpg\" alt=\"Flock of sheep.\" style=\"height: 150px; margin: 20px; float: left\"/>\n",
    "</div>\n",
    "    \n",
    "Aye captain, that's the warp we're talking about! Sorry if you thought a warp was something cooler and respect for the sheep population.\n",
    "\n",
    "In CUDA terms, a **warp** is a unit consisting of **32 threads** that are execuded on a single CUDA core. The threads therein all execute the same instruction set. Regardless of how many threads are defined in a kernel function call, a multiple of 32 threads is always assigned to it. Unless explicityl defined during the kernel function call, the redundant threads - also called idle threads - don't contribute to the computing. The below figure illustrates a stream multiprocessor (SM) in the Nvidia Fermi architecture.\n",
    "\n",
    "<img src=\"./Image/multiprocessor.png\" alt=\"Fermi stream multiprocessor.\" style=\"width: 600px; margin: 20px;\"/>\n",
    "\n",
    "Other architectures might have different layouts and numbers of individual components, but what unites them is the fact that they all have CUDA cores and they offer the most throughput when entire warps are utilized. To evaluate what happens, we need to run over multiple thread-block configurations and time the kernel. Below is a basic example of how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 10\n",
    "execution_times = []\n",
    "\n",
    "for i in range(32):\n",
    "    t = sync_timeit( lambda: kernel_2D[(4, 4), (1+i, 1+i)](image_array_gs, xmin, xmax, histogram_out), number=10)\n",
    "    execution_times.append(t)\n",
    "\n",
    "plt.plot( [10+i for i in range(len(execution_times))], execution_times )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-daughter",
   "metadata": {},
   "source": [
    "Unsurprisingly, increasing the amount of threads resuces the processing time. However, there is some jitter-like inconsistency present in the results. A more detailed study is needed to fully assess the dependency between execution times and thread count, while taking into account warp utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-director",
   "metadata": {},
   "source": [
    "## Assignment - Generating an image\n",
    "\n",
    "\n",
    "### Helpers and pointers\n",
    "\n",
    "Your goal is to generate a 2D image on the GPU, such as the one described by:\n",
    "\n",
    "$px[i, j] = (\\sin{\\dfrac{i \\cdot 2.0 \\pi}{T}} + 1.0) \\cdot (\\sin{\\dfrac{j \\cdot 2.0 \\pi}{T}} + 1.0) \\cdot \\dfrac{1}{4}$\n",
    "\n",
    "\n",
    "### Report and deadline\n",
    "\n",
    "*Follow the bellow insructions and write a report when done. Explain what you have learned and critically assess the performance of your code. The report shouldn't exceed 3 pages (including figures). If you wish to share your code, include a link to the corresponding repository in your report or write the report in the form of a Notebook.*\n",
    "\n",
    "*Hand in your reports to alexander.marinsek@kuleuven.be by:*\n",
    "* *SA & GS: Sunday, xx/xx/2021, 23:59 CExT*\n",
    "* *ES: Sunday, xx/xx/2021, 23:59 CExT*\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "##### I. Striding\n",
    "\n",
    "1. Design an image-generating kernel for 1024 px square images using the equation \n",
    "\n",
    "$px[i, j] = (\\sin{\\dfrac{i \\cdot 2.0 \\pi}{T}} + 1.0) \\cdot (\\sin{\\dfrac{j \\cdot 2.0 \\pi}{T}} + 1.0) \\cdot \\dfrac{1}{4}$\n",
    "\n",
    "2. Test it using 1024$^2$ threads. Choose the block size and number of threads per block accordingly. Keep in mind the threads per block limit (1024 on the GT 730). \n",
    "3. Now use only a quarter of the threads - without striding - and observe the plotted image. What hapens, and what would happen if you had too many threads?\n",
    "4. Overcome the problem using striding - one thread may process multiple pixels.\n",
    "5. Run it again and verify the entire image is rendered. Graphically and in words, how would you describe striding?\n",
    "\n",
    "\n",
    "##### II. Leveraging warps \n",
    "\n",
    "1. Evaluate how the kernel behaves when increasing the total number of threads and the amount of threads per block. \n",
    "2. Plot and discuss the dependency of the computational time on the number of threads. Are there any peculiarities and pitfalls one must keep in mind when running a kernel function?\n",
    "3. (optional) Insert the statement `histogram_out_device = cuda.to_device(histogram_out)` before calling the kernel and this time pass it the `histogram_out_device` array. What happens and how is it manifested on the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-sullivan",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "* (Stride loop) Introduction to Parallel Programming with CUDA and C++: https://medium.com/@avinkavish/introduction-to-parallel-programming-with-cuda-and-c-ff2ca339aca\n",
    "* (Stream multiprocessor architecture) Computer Organization and Architecture, chapter 19.3: http://williamstallings.com/COA/\n",
    "* (Lenna) Story behind the photo and full pic: https://datafireball.com/2016/09/24/lena-the-origin-about-the-de-facto-test-image/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
